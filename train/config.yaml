# LoRA fine-tune config for training your own cheat assistant LLM
base_model: mistralai/Mistral-7B-Instruct-v0.1
load_in_4bit: true
bnb_4bit_compute_dtype: float16
bnb_4bit_quant_type: nf4
use_double_nested_lora: false

dataset:
  train_path: "train/data.json"
  type: "alpaca"

learning_rate: 5e-5
micro_batch_size: 2
num_epochs: 3
save_every: 100
output_dir: "train/output"
resume_from_checkpoint: null

trainer:
  fp16: true
  gradient_checkpointing: true
  logging_steps: 10
  save_strategy: "steps"
  save_steps: 100
